{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2858c05c",
   "metadata": {},
   "source": [
    "# JABS Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b472e4",
   "metadata": {},
   "source": [
    "# Purpose/Expectations\n",
    "\n",
    "The contents of this course will grant insight into important characteristics while trainig behavioral classifiers. The usage of JABS code in this tutorial is strictly limited to reading in project annotations and feature vectors.\n",
    "\n",
    "Within this tutorial, you will be provided with 2 example annotated projects that contain sparse labels that will be used for training and dense labels that you will use for final evaluation of your best model.\n",
    "\n",
    "Expected knowledge before coming doing the course\n",
    "* Familiarity with animal pose data and frame-level features derived from that data\n",
    "* Basic python experience\n",
    "* Some numpy experience\n",
    "* Familiarity with generating plots in python (preference to using plotnine, which is ggplot-style syntax)\n",
    "\n",
    "Please feel free to use the \"solution\" links when the programming experience is above your knowledge.\n",
    "\n",
    "Expected takeaways of the course\n",
    "* Learn the core process to train a frame-wise predictor of behavior\n",
    "* Become familiar with key components to take into account when training these types of classifiers\n",
    "* Gain insight for some of the design decision considerations our group integrated into the JABS software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99caad43",
   "metadata": {},
   "source": [
    "## Step 0\n",
    "Import a lot of libraries that we will be using in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import sklearn\n",
    "# JABS install directory\n",
    "import sys\n",
    "sys.path.append('JABS-behavior-classifier/')\n",
    "from src.project import Project\n",
    "from src.classifier import Classifier\n",
    "# Plotting library\n",
    "import plotnine as p9\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064c25c",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Import the project annotations that we are providing\n",
    "These annotations were created using our JABS software, so we can use that library to extract the annotations in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8146ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project('JABS-Training-Data/')\n",
    "# We're picking the first behavior in the project. Each JABS project can contain multiple behavior annotations.\n",
    "behavior = project.load_metadata()['behaviors'][0]\n",
    "# Here we read in the all important information for all labels\n",
    "training_data = project.get_labeled_features(behavior, window_size=5, use_social_features=True)\n",
    "# Since this is more of an internal format, we can separate out the data to be more easily accessible\n",
    "all_labels = training_data[0]['labels']\n",
    "annotation_animal = training_data[0]['groups']\n",
    "annotation_animal_names = [training_data[1][x]['video'] + ' animal ' + str(training_data[1][x]['identity']) for x in annotation_animal]\n",
    "feature_names = training_data[0]['column_names']\n",
    "all_features = Classifier().combine_data(training_data[0]['per_frame'],training_data[0]['window'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa04bdc",
   "metadata": {},
   "source": [
    "Also extract some bout-level groups, something that JABS doesn't normally do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53585907",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_annotations = [project.load_video_labels(training_data[1][x]['video']).as_dict()['labels'][str(training_data[1][x]['identity'])][behavior] for x in np.unique(annotation_animal)]\n",
    "num_animals_annotated = len(raw_annotations)\n",
    "bout_data = [[np.repeat(animal_idx+bout_idx*num_animals_annotated, bout['end']-bout['start']+1) for bout_idx, bout in enumerate(animal_data)] for animal_idx, animal_data in enumerate(raw_annotations)]\n",
    "bout_data = np.concatenate(list(chain.from_iterable(bout_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11d080",
   "metadata": {},
   "source": [
    "At this point we have extracted feature data and annotations from a JABS project.\n",
    "Here's a description of important variables:\n",
    "* `all_features` contains the matrix of all the features. The first dimension is the frame index and the second dimension is the feature index.\n",
    "* `all_labels` contains a vector of labels. 0 = not behavior, 1 = behavior\n",
    "* `annotation_animal` contains a vector with a unique number for each animal\n",
    "* `bout_data` contains a vector with a unique bout number for each bout annotated\n",
    "* `feature_names` contains the name of each feature. Each value here is the name of the 2nd dimension of the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684fee9",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Read in the held-out test set\n",
    "\n",
    "This set is distinct from the validation dataset we'll be splitting in 2 key factors:\n",
    "1. This set will be held out and should not be used in tuning model parameters.\n",
    "2. It is also densely annotated (all frames contain a label), rather than the sparse annotation provided for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52487423",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_project = Project('JABS-Test-Data/')\n",
    "test_data = test_project.get_labeled_features(behavior, window_size=5, use_social_features=True)\n",
    "test_labels = test_data[0]['labels']\n",
    "test_animals = test_data[0]['groups']\n",
    "test_features = Classifier().combine_data(test_data[0]['per_frame'],test_data[0]['window'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7bdfb9",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Inspect characteristics of the dataset\n",
    "Since the annotations are located in `all_labels`, we should inspect some characteristics of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e405aae",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "How many annotations do we have?\n",
    "[Solution](http://localhost:8888/notebooks/ML-Course-2022-JABS_SOLUTIONS.ipynb#Question-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f663ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c2f820",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "How many labels do we have of each class: 0 (not-behavior) and 1 (behavior)\n",
    "[Solution](http://localhost:8888/notebooks/ML-Course-2022-JABS_SOLUTIONS.ipynb#Question-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d40c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_not_behavior = sum(all_labels==0)\n",
    "print(num_not_behavior)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf849a9a",
   "metadata": {},
   "source": [
    "### Question 3-4\n",
    "How many bouts were annotated?\n",
    "\n",
    "How many animals were annotated?\n",
    "[Solution](http://localhost:8888/notebooks/ML-Course-2022-JABS_SOLUTIONS.ipynb#Question-3-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ffdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bouts = len(np.unique(bout_data))\n",
    "print(num_bouts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5b75e",
   "metadata": {},
   "source": [
    "## Discussion 1\n",
    "\n",
    "What are characteristics of the dataset that may be important about creating a good model?\n",
    "\n",
    "Hypotheticals to think about:\n",
    "* If you have a rare behavior, is it okay to label a lot more \"not behavior\"? What might the model try and do to improve accuracy if that ratio becomes 1000:1?\n",
    "* If one animal tends to express the behavior more than another, is it okay to provide more labels on from that individual?\n",
    "\n",
    "[Discussion Notes](http://localhost:8888/notebooks/ML-Course-2022-JABS_SOLUTIONS.ipynb#Discussion-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a66dfa",
   "metadata": {},
   "source": [
    "## Extra visualizations of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dcf0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_df = pd.DataFrame({'annotations':all_labels, 'bout':bout_data, 'animal':annotation_animal, 'annotation_idx':np.arange(len(all_labels))})\n",
    "# Histogram of the class labels\n",
    "plot_labels = p9.ggplot() + \\\n",
    "        p9.geom_histogram(mapping=p9.aes(x='factor(annotations)'), data=plotting_df, bins=2, fill='#9e9e9e', color='#000000', size=2) + \\\n",
    "        p9.labs(title='Class Labels', x='Class', y='Count') + \\\n",
    "        p9.scale_x_discrete(labels=['Not-Behavior',behavior]) + \\\n",
    "        p9.theme_bw()\n",
    "plot_labels.draw().show()\n",
    "\n",
    "# Class labels per-animal\n",
    "plot_animals = p9.ggplot() + \\\n",
    "        p9.geom_histogram(mapping=p9.aes(x='factor(annotations)'), data=plotting_df, bins=2, fill='#9e9e9e', color='#000000', size=2) + \\\n",
    "        p9.labs(title='Class Labels by Animal', x='Class', y='Count') + \\\n",
    "        p9.scale_x_discrete(labels=['Not-Behavior',behavior]) + \\\n",
    "        p9.facet_wrap('animal') + \\\n",
    "        p9.theme_bw()\n",
    "plot_animals.draw().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f80eef4",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Splitting the training data into train and validation\n",
    "\n",
    "We will be using the training portion of the data to allow the algorithm to learn the best parameters to make predictions\n",
    "\n",
    "The validation will be held out to evaluate performance of the classifier\n",
    "\n",
    "Here we define a handful of functions that accepts the features and labels and returns the split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19820153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to split the data\n",
    "# Here we're going to manually shuffle and split\n",
    "# Naive approach 1 - random sorted split (deterministic)\n",
    "def split_data(features, labels, percent_train=0.75):\n",
    "    available_examples = np.arange(len(labels))\n",
    "    train_idxs = available_examples[:int(len(labels)*percent_train)]\n",
    "    test_idxs = available_examples[int(len(labels)*percent_train):]\n",
    "    # Separate out the training data\n",
    "    train_features = features[train_idxs]\n",
    "    train_labels = labels[train_idxs]\n",
    "    # Separate out the validation data\n",
    "    valid_features = features[test_idxs]\n",
    "    valid_labels = labels[test_idxs]\n",
    "    return train_features, train_labels, valid_features, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da60fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive approach 2 - random shuffle\n",
    "def random_split_data(features, labels, percent_train=0.75):\n",
    "    available_examples = np.arange(len(labels))\n",
    "    np.random.shuffle(available_examples)\n",
    "    train_idxs = available_examples[:int(len(labels)*percent_train)]\n",
    "    test_idxs = available_examples[int(len(labels)*percent_train):]\n",
    "    # Separate out the training data\n",
    "    train_features = features[train_idxs]\n",
    "    train_labels = labels[train_idxs]\n",
    "    # Separate out the validation data\n",
    "    valid_features = features[test_idxs]\n",
    "    valid_labels = labels[test_idxs]\n",
    "    return train_features, train_labels, valid_features, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c3e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn to split the data\n",
    "# Stratified splitting:\n",
    "# Attempts to preserve the train/valid class representations\n",
    "def sklearn_stratified_split(features, labels, percent_train=0.75):\n",
    "    train_idxs, test_idxs = list(sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, train_size=percent_train).split(features, labels))[0]\n",
    "    # Separate out the training data\n",
    "    train_features = features[train_idxs]\n",
    "    train_labels = labels[train_idxs]\n",
    "    # Separate out the validation data\n",
    "    valid_features = features[test_idxs]\n",
    "    valid_labels = labels[test_idxs]\n",
    "    return train_features, train_labels, valid_features, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f40ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group splitting\n",
    "# Leaves one group out within the split\n",
    "# Note that this one group that is left out is not guaranteed to contain both labels, so sometime performance will contain things like division by 0.\n",
    "def sklearn_logo_split(features, labels, groups, percent_train=0.75):\n",
    "    train_idxs, test_idxs = list(sklearn.model_selection.LeaveOneGroupOut().split(features, labels, groups))[0]\n",
    "    # Separate out the training data\n",
    "    train_features = features[train_idxs]\n",
    "    train_labels = labels[train_idxs]\n",
    "    # Separate out the validation data\n",
    "    valid_features = features[test_idxs]\n",
    "    valid_labels = labels[test_idxs]\n",
    "    return train_features, train_labels, valid_features, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de695fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group splitting #2\n",
    "# Leaves multiple groups out with a target percent annotations that fall into the training set\n",
    "def sklearn_group_split(features, labels, groups, percent_train=0.75):\n",
    "    train_idxs, test_idxs = list(sklearn.model_selection.GroupShuffleSplit(n_splits=1, train_size=percent_train).split(features, labels, groups))[0]\n",
    "    # Separate out the training data\n",
    "    train_features = features[train_idxs]\n",
    "    train_labels = labels[train_idxs]\n",
    "    # Separate out the validation data\n",
    "    valid_features = features[test_idxs]\n",
    "    valid_labels = labels[test_idxs]\n",
    "    return train_features, train_labels, valid_features, valid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42ae40",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "Inspect/Visualize the effects of different splits of the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee97d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split\n",
    "train_features, train_labels, valid_features, valid_labels = random_split_data(all_features, all_labels)\n",
    "# Stratified split\n",
    "#train_features, train_labels, valid_features, valid_labels =sklearn_stratified_split(all_features, all_labels)\n",
    "# Leave one group out split (bout)\n",
    "#train_features, train_labels, valid_features, valid_labels = sklearn_logo_split(all_features, all_labels, bout_data)\n",
    "# Leave one group out split (animal)\n",
    "#train_features, train_labels, valid_features, valid_labels = sklearn_logo_split(all_features, all_labels, annotation_animal)\n",
    "# Leave multiple groups out (bouts)\n",
    "#train_features, train_labels, valid_features, valid_labels = sklearn_group_split(all_features, all_labels, bout_data)\n",
    "# Leave multiple groups out (animals)\n",
    "#train_features, train_labels, valid_features, valid_labels = sklearn_group_split(all_features, all_labels, annotation_animal)\n",
    "\n",
    "train_df = pd.DataFrame({'state':'train', 'label':train_labels})\n",
    "valid_df = pd.DataFrame({'state':'valid', 'label':valid_labels})\n",
    "plot_df = pd.concat([train_df, valid_df])\n",
    "\n",
    "split_plot = p9.ggplot(plot_df) + \\\n",
    "    p9.geom_bar(p9.aes(x='state', fill='factor(label)')) + \\\n",
    "    p9.theme_bw()\n",
    "\n",
    "split_plot.draw().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43964850",
   "metadata": {},
   "source": [
    "## Label Balancing\n",
    "\n",
    "Since we're aware that we have roughly a 3x more annotations for not-behavior, we should probably consider balancing the labels\n",
    "\n",
    "We can take multiple approaches to creating a more balanced dataset:\n",
    "1. Downsampling: Discard data from the class which has the most annotation until the labels are balanced\n",
    "2. Upsampling: Duplicate data from the class which has the least annotation until the labels are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11853e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsamples the featuers, labels, and groups such that the labels are balanced\n",
    "def downsample_balanced(features, labels, groups=None):\n",
    "    _, label_counts = np.unique(labels, return_counts=True)\n",
    "    # Since we want to discard samples, we should pick the smaller of the 2 class sizes\n",
    "    smallest_class_count = np.min(label_counts)\n",
    "    class_0_idxs = np.where(labels==0)[0]\n",
    "    class_1_idxs = np.where(labels==1)[0]\n",
    "    # Here we can randomly choose without replacement examples\n",
    "    class_0_idxs = np.random.choice(class_0_idxs, smallest_class_count, replace=False)\n",
    "    class_1_idxs = np.random.choice(class_1_idxs, smallest_class_count, replace=False)\n",
    "    selected_samples = np.sort(np.concatenate([class_0_idxs, class_1_idxs]))\n",
    "    new_features = features[selected_samples,:]\n",
    "    new_labels = labels[selected_samples]\n",
    "    if groups is not None:\n",
    "        new_groups = groups[selected_samples]\n",
    "        return new_features, new_labels, new_groups\n",
    "    else:\n",
    "        return new_features, new_labels, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82ef1e",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Write a function to upsample the data\n",
    "[Solution](http://localhost:8888/notebooks/ML-Course-2022-JABS_SOLUTIONS.ipynb#Question-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsamples the featuers, labels, and groups such that the labels are balanced\n",
    "def upsample_balanced(features, labels, groups=None):\n",
    "    _, label_counts = np.unique(labels, return_counts=True)\n",
    "    # If for downsampling, we want the smallest class count, what do we want to use for upsampling?\n",
    "    \n",
    "    class_0_idxs = np.where(labels==0)[0]\n",
    "    class_1_idxs = np.where(labels==1)[0]\n",
    "    # Decide how to sample the data here\n",
    "    \n",
    "    selected_samples = np.sort(np.concatenate([class_0_idxs, class_1_idxs]))\n",
    "    new_features = features[selected_samples,:]\n",
    "    new_labels = labels[selected_samples]\n",
    "    if groups is not None:\n",
    "        new_groups = groups[selected_samples]\n",
    "        return new_features, new_labels, new_groups\n",
    "    else:\n",
    "        return new_features, new_labels, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd11ce7",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "\n",
    "Inspect/Visualize the effects of downsampling the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "balanced_features, balanced_labels, balanced_groups = downsample_balanced(all_features, all_labels, annotation_animal)\n",
    "# Upsample\n",
    "#balanced_features, balanced_labels, balanced_groups = upsample_balanced(all_features, all_labels, annotation_animal)\n",
    "train_features, train_labels, valid_features, valid_labels = sklearn_group_split(balanced_features, balanced_labels, balanced_groups)\n",
    "train_df = pd.DataFrame({'state':'train', 'label':train_labels})\n",
    "valid_df = pd.DataFrame({'state':'valid', 'label':valid_labels})\n",
    "plot_df = pd.concat([train_df, valid_df])\n",
    "\n",
    "split_plot = p9.ggplot(plot_df) + \\\n",
    "    p9.geom_bar(p9.aes(x='state', fill='factor(label)')) + \\\n",
    "    p9.theme_bw()\n",
    "\n",
    "split_plot.draw().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1de99",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Train classifiers\n",
    "\n",
    "For the purposes of this tutorial, we will be relying on using SKLearn to training classifiers.\n",
    "\n",
    "Good recommended classifiers:\n",
    "1. [Adaboost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn-ensemble-adaboostclassifier)\n",
    "2. [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn-ensemble-randomforestclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8dd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how to train a decision tree classifier\n",
    "def train_dt_classifier(train_features, train_labels):\n",
    "    # Create a classifier object\n",
    "    # Note here we are using a bunch of default values that SKLearn has picked for us.\n",
    "    # Check the documentation of the various parameters you can adjust for the decision trees:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn-tree-decisiontreeclassifier\n",
    "    classifier = sklearn.tree.DecisionTreeClassifier()\n",
    "    # Train the classifier on our training features + labels\n",
    "    classifier = classifier.fit(train_features, train_labels)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187b75d",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Write functions to train an adaboost and random forest classifier\n",
    "[Solution](http://localhost:8888/notebooks/ML-Course-2022-JABS_SOLUTIONS.ipynb#Question-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost Classifier\n",
    "def train_ada_classifier(train_features, train_labels):\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d299383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest Classifier\n",
    "def train_rf_classifier(train_features, train_labels):\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8a4d1",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "\n",
    "Frame-wise evaluation of the classifier\n",
    "\n",
    "Since our classifier predicts for every frame, we can estimate its performance by comparing the held-out validation data with the predictions it is making.\n",
    "\n",
    "For evaluating classifier performance, we must first calculate the 4 values of a confusion matrix:\n",
    "1. True positives (TP): When both the classifier and the ground truth assign \"behavior\"\n",
    "2. False negatives (FN): When the classifier predicts \"not behavior\" and the ground truth assigns \"behavior\"\n",
    "3. False positives (FP): When the classifier predicts \"behavior\" and the ground truth assigns \"not behavior\"\n",
    "4. True negatives (TN): When both the classifier and the ground truth assign \"not behavior\"\n",
    "\n",
    "From these, we can calculate 4 additional useful performance metrics for describing the classifier. Wikipedia has a [good article](https://en.wikipedia.org/wiki/Precision_and_recall) outlining these as well as more, but these are the 4 most common.\n",
    "1. Accuracy: The total number of predictions that were correct.\n",
    "    * (TP+TN)/(TP+TN+FP+FN)\n",
    "2. Precision: Of the predictions for \"behavior\", how many were correct?\n",
    "    * TP/(TP+FP)\n",
    "3. Recall: Of the ground truths labeled \"behavior\", how many did the classifier predict?\n",
    "    * TP/(TP+FN)\n",
    "4. F-beta (F1, F-score): Harmonic mean of precision and recall \n",
    "    * 2 * (Pr * Re)/(Pr+Re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb057ba",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Complete the definitions of the confusion matrix and the performance metrics below\n",
    "[Solution](http://localhost:8888/notebooks/ML-Course-2022-JABS_SOLUTIONS.ipynb#Question-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for evaluating the classifier performance\n",
    "def eval_classifier_performance(classifier, features, labels, print_results=True):\n",
    "    # Predict on held-out set\n",
    "    predictions = classifier.predict(features)\n",
    "    # Accuracy\n",
    "    acc = np.mean(predictions == labels)\n",
    "    # Confusion matrix values for a binary classifier\n",
    "    tp_count = np.sum(np.logical_and(predictions==1, labels==1))\n",
    "    tn_count = \n",
    "    fp_count = \n",
    "    fn_count = \n",
    "    # Calculate other metrics\n",
    "    # Of the predictions we made, how many were correct\n",
    "    precision = \n",
    "    # Of the things we were looking for, how many did we find\n",
    "    recall = \n",
    "    # Harmonic mean of Pr and Re\n",
    "    f1 = 2*(precision * recall)/(precision + recall)\n",
    "    if print_results:\n",
    "        print('Accuracy: ' + str(acc))\n",
    "        print('Precision: ' + str(precision))\n",
    "        print('Recall: ' + str(recall))\n",
    "        print('F1-score: ' + str(f1))\n",
    "    # We can also show that sklearn has tools for providing these metrics\n",
    "    # Note that we've calculated the \"precision\" and \"recall\" for the behavior (value == 1)\n",
    "    # print(sklearn.metrics.classification_report(labels, predictions))\n",
    "    # Manually obtaining each\n",
    "    # sklearn.metrics.precision_score(labels, predictions)\n",
    "    # sklearn.metrics.recall_score(labels, predictions)\n",
    "    # sklearn.metrics.f1_score(labels, predictions)\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89a7d9",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "\n",
    "Run through the steps:\n",
    "1. Split the training data into train/valid\n",
    "2. Train a classifier on the train portion of the split\n",
    "3. Evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the data into train/valid\n",
    "train_features, train_labels, valid_features, valid_labels = random_split_data(all_features, all_labels)\n",
    "# Train a classifier\n",
    "dt_classifier = train_dt_classifier(train_features, train_labels)\n",
    "# Evaluate the classifier\n",
    "eval_classifier_performance(dt_classifier, valid_features, valid_labels)\n",
    "# Repeat these steps to test how different parameters influence performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22cbb7",
   "metadata": {},
   "source": [
    "## Experiment 3\n",
    "\n",
    "See performance of different classifiers and sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d21e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling options:\n",
    "# Random split\n",
    "#train_features, train_labels, valid_features, valid_labels = random_split_data(all_features, all_labels)\n",
    "# Stratified split\n",
    "#train_features, train_labels, valid_features, valid_labels =sklearn_stratified_split(all_features, all_labels)\n",
    "# Leave one group out split (bout)\n",
    "#train_features, train_labels, valid_features, valid_labels = sklearn_logo_split(all_features, all_labels, bout_data)\n",
    "# Leave one group out split (animal)\n",
    "#train_features, train_labels, valid_features, valid_labels = sklearn_logo_split(all_features, all_labels, annotation_animal)\n",
    "# Leave multiple groups out (bouts)\n",
    "#train_features, train_labels, valid_features, valid_labels = sklearn_group_split(all_features, all_labels, bout_data)\n",
    "# Leave multiple groups out (animals)\n",
    "#train_features, train_labels, valid_features, valid_labels = sklearn_group_split(all_features, all_labels, annotation_animal)\n",
    "# Example with extra step for balanced labels + group sampling\n",
    "balanced_features, balanced_labels, balanced_groups = downsample_balanced(all_features, all_labels, annotation_animal)\n",
    "train_features, train_labels, valid_features, valid_labels = sklearn_group_split(balanced_features, balanced_labels, balanced_groups)\n",
    "\n",
    "# Classifier options:\n",
    "# Decision Tree\n",
    "#classifier = train_dt_classifier(train_features, train_labels)\n",
    "# Ada Boost\n",
    "#classifier = train_ada_classifier(train_features, train_labels)\n",
    "# Random Forest\n",
    "classifier = train_rf_classifier(train_features, train_labels)\n",
    "\n",
    "# Run the evaluation!\n",
    "eval_classifier_performance(classifier, valid_features, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8164766",
   "metadata": {},
   "source": [
    "## Experiment 4\n",
    "\n",
    "Expand the search of parameters by also adjusting the parameters of the model, eg the number of trees in the random forest.\n",
    "\n",
    "We provide an example structure for looping through 10 training splits for the decision tree classifier.\n",
    "\n",
    "Suggested tests:\n",
    "1. Change the classifier type\n",
    "2. Change the train/valid split approach\n",
    "3. Include balanced data\n",
    "4. Test various parameters of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e508576",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "# random splits\n",
    "for test_model in np.arange(10):\n",
    "    train_features, train_labels, valid_features, valid_labels = random_split_data(all_features, all_labels)\n",
    "    dt_classifier = train_dt_classifier(train_features, train_labels)\n",
    "    acc, pr, re, f1 = eval_classifier_performance(dt_classifier, valid_features, valid_labels, print_results=False)\n",
    "    all_results.append(pd.DataFrame({'classifier':['decision tree'], 'split_method':['random'], 'accuracy':[acc], 'precision':[pr], 'recall':[re], 'f1':[f1]}))\n",
    "    ada_classifier = train_ada_classifier(train_features, train_labels)\n",
    "    acc, pr, re, f1 = eval_classifier_performance(ada_classifier, valid_features, valid_labels, print_results=False)\n",
    "    all_results.append(pd.DataFrame({'classifier':['ada boost'], 'split_method':['random'], 'accuracy':[acc], 'precision':[pr], 'recall':[re], 'f1':[f1]}))\n",
    "    rf_classifier = train_rf_classifier(train_features, train_labels)\n",
    "    acc, pr, re, f1 = eval_classifier_performance(rf_classifier, valid_features, valid_labels, print_results=False)\n",
    "    all_results.append(pd.DataFrame({'classifier':['random forest'], 'split_method':['random'], 'accuracy':[acc], 'precision':[pr], 'recall':[re], 'f1':[f1]}))\n",
    "\n",
    "# leave bouts out splits\n",
    "for test_model in np.arange(10):\n",
    "    train_features, train_labels, valid_features, valid_labels = sklearn_group_split(all_features, all_labels, bout_data)\n",
    "    dt_classifier = train_dt_classifier(train_features, train_labels)\n",
    "    acc, pr, re, f1 = eval_classifier_performance(dt_classifier, valid_features, valid_labels, print_results=False)\n",
    "    all_results.append(pd.DataFrame({'classifier':['decision tree'], 'split_method':['leave bouts out'], 'accuracy':[acc], 'precision':[pr], 'recall':[re], 'f1':[f1]}))\n",
    "    ada_classifier = train_ada_classifier(train_features, train_labels)\n",
    "    acc, pr, re, f1 = eval_classifier_performance(ada_classifier, valid_features, valid_labels, print_results=False)\n",
    "    all_results.append(pd.DataFrame({'classifier':['ada boost'], 'split_method':['leave bouts out'], 'accuracy':[acc], 'precision':[pr], 'recall':[re], 'f1':[f1]}))\n",
    "    rf_classifier = train_rf_classifier(train_features, train_labels)\n",
    "    acc, pr, re, f1 = eval_classifier_performance(rf_classifier, valid_features, valid_labels, print_results=False)\n",
    "    all_results.append(pd.DataFrame({'classifier':['random forest'], 'split_method':['leave bouts out'], 'accuracy':[acc], 'precision':[pr], 'recall':[re], 'f1':[f1]}))\n",
    "\n",
    "# leave animals out splits\n",
    "for test_model in np.arange(10):\n",
    "    train_features, train_labels, valid_features, valid_labels = sklearn_group_split(all_features, all_labels, annotation_animal)\n",
    "    dt_classifier = train_dt_classifier(train_features, train_labels)\n",
    "    acc, pr, re, f1 = eval_classifier_performance(dt_classifier, valid_features, valid_labels, print_results=False)\n",
    "    all_results.append(pd.DataFrame({'classifier':['decision tree'], 'split_method':['leave animal out'], 'accuracy':[acc], 'precision':[pr], 'recall':[re], 'f1':[f1]}))\n",
    "    ada_classifier = train_ada_classifier(train_features, train_labels)\n",
    "    acc, pr, re, f1 = eval_classifier_performance(ada_classifier, valid_features, valid_labels, print_results=False)\n",
    "    all_results.append(pd.DataFrame({'classifier':['ada boost'], 'split_method':['leave animal out'], 'accuracy':[acc], 'precision':[pr], 'recall':[re], 'f1':[f1]}))\n",
    "    rf_classifier = train_rf_classifier(train_features, train_labels)\n",
    "    acc, pr, re, f1 = eval_classifier_performance(rf_classifier, valid_features, valid_labels, print_results=False)\n",
    "    all_results.append(pd.DataFrame({'classifier':['random forest'], 'split_method':['leave animal out'], 'accuracy':[acc], 'precision':[pr], 'recall':[re], 'f1':[f1]}))\n",
    "   \n",
    "# Flatten the list of dataframes into one\n",
    "results_df = pd.concat(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293f670",
   "metadata": {},
   "source": [
    "Inspect the results of your scan(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = pd.melt(results_df, id_vars = ['classifier','split_method'], value_vars = ['accuracy','precision','recall','f1'])\n",
    "performance_plot = p9.ggplot(data=melted_df) + \\\n",
    "    p9.geom_point(p9.aes(x='classifier', y='value', color='variable'), position=p9.position_dodge(width=0.4)) + \\\n",
    "    p9.facet_wrap('split_method') + \\\n",
    "    p9.labs(title='Classifier accuracy', x='Classifier type', y='Metric') + \\\n",
    "    p9.theme_bw()\n",
    "\n",
    "performance_plot.draw().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b001f",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "\n",
    "Evaluate your best classifier on the held-out test dataset\n",
    "\n",
    "You could also potentially just train a classifier on the entire training set (after you've tuned the numbers with the validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41365d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_classifier = dt_classifier\n",
    "best_classifier = train_rf_classifier(all_features, all_labels)\n",
    "eval_classifier_performance(best_classifier, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0bbce",
   "metadata": {},
   "source": [
    "You may observe that performance has dropped a bit, but if you selected the correct train/valid split approach and didn't over-fit on your validation data, it should still perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383acb5b",
   "metadata": {},
   "source": [
    "## Discussion 2\n",
    "\n",
    "Up until now, we've been focussing on frame-level agreement, which is simple enough for training classifiers.\n",
    "\n",
    "Are there better ways to evaluate performance of a behavior classifier?\n",
    "\n",
    "[Discussion Notes](http://localhost:8888/notebooks/ML-Course-2022-JABS_SOLUTIONS.ipynb#Discussion-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d910cb2",
   "metadata": {},
   "source": [
    "## Step 9\n",
    "\n",
    "Bout-level agreement on dense annotation\n",
    "\n",
    "In order to measure bout-level agreement, we need to transform the data into starts and ends\n",
    "\n",
    "We can either write the detection of bouts within predictions manually or re-use a classical compression algorithm called run length encoding\n",
    "\n",
    "Note that typical RLE algorithms will encode all states (both \"behavior\" and \"not-behavior\"), while we primarily care about only 1 of the 2 states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run length encoding, implemented using numpy\n",
    "# Accepts a 1d vector\n",
    "# Returns a tuple containing (starts, durations, values)\n",
    "def rle(inarray):\n",
    "    ia = np.asarray(inarray)\n",
    "    n = len(ia)\n",
    "    if n == 0: \n",
    "        return (None, None, None)\n",
    "    else:\n",
    "        y = ia[1:] != ia[:-1]\n",
    "        i = np.append(np.where(y), n - 1)\n",
    "        z = np.diff(np.append(-1, i))\n",
    "        p = np.cumsum(np.append(0, z))[:-1]\n",
    "        return(p, z, ia[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5228a6",
   "metadata": {},
   "source": [
    "Use the RLE algorithm to encode predictions\n",
    "\n",
    "We also need to separate the data by animal, because bouts can only exist within animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f83a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = best_classifier.predict(test_features)\n",
    "\n",
    "# Arrays to store the list of bouts by animal\n",
    "all_pr_bouts = []\n",
    "all_gt_bouts = []\n",
    "# Loop over each animal and add their bouts to the lists\n",
    "for animal_id in np.unique(test_animals):\n",
    "    animal_indices = test_animals==animal_id\n",
    "    test_bout_predictions = rle(test_predictions[animal_indices])\n",
    "    # Only store the bouts of behavior\n",
    "    behavior_bouts = test_bout_predictions[2]==1\n",
    "    if np.any(behavior_bouts):\n",
    "        test_bout_predictions = (test_bout_predictions[0][behavior_bouts], test_bout_predictions[1][behavior_bouts])\n",
    "        all_pr_bouts.append(test_bout_predictions)\n",
    "    else:\n",
    "        all_pr_bouts.append(([],[]))\n",
    "    test_bout_gt = rle(test_labels[animal_indices])\n",
    "    # Only store the bouts of behavior\n",
    "    behavior_bouts = test_bout_gt[2]==1\n",
    "    if np.any(behavior_bouts):\n",
    "        test_bout_gt = (test_bout_gt[0][behavior_bouts], test_bout_gt[1][behavior_bouts])\n",
    "        all_gt_bouts.append(test_bout_gt)\n",
    "    else:\n",
    "        all_gt_bouts.append(([],[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a864327",
   "metadata": {},
   "source": [
    "Display the predictions next to their GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e347a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bout_data = []\n",
    "for animal_idx, animal_id in enumerate(np.unique(test_animals)):\n",
    "    gt_bout_data = all_gt_bouts[animal_idx]\n",
    "    if len(gt_bout_data[0])>0:\n",
    "        gt_bout_df = pd.DataFrame({'state':'GT', 'animal':animal_id, 'animal_idx':animal_idx, 'start_time':gt_bout_data[0], 'end_time':gt_bout_data[0]+gt_bout_data[1]})\n",
    "        bout_data.append(gt_bout_df)\n",
    "    pr_bout_data = all_pr_bouts[animal_idx]\n",
    "    if len(pr_bout_data[0])>0:\n",
    "        pr_bout_df = pd.DataFrame({'state':'PR', 'animal':animal_id, 'animal_idx':animal_idx, 'start_time':pr_bout_data[0], 'end_time':pr_bout_data[0]+pr_bout_data[1]})\n",
    "        bout_data.append(pr_bout_df)\n",
    "\n",
    "bout_data = pd.concat(bout_data)\n",
    "bout_plot = p9.ggplot(bout_data) + \\\n",
    "    p9.geom_rect(p9.aes(xmin='start_time', xmax='end_time', ymin='animal_idx-0.25', ymax='animal_idx+0.25', fill='factor(state)'), alpha=0.5) + \\\n",
    "    p9.labs(title='Bout annotations and predictions', x='Time, frame', y='Animal index', fill='State') + \\\n",
    "    p9.theme_bw()\n",
    "\n",
    "bout_plot.draw().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2c143",
   "metadata": {},
   "source": [
    "## Step 10\n",
    "\n",
    "Strategies for evaluating performance of bout-based metrics\n",
    "\n",
    "Since predictions are almost never going to be identical to the ground truth, we need to adjust our strategy for calling correct and incorrect classifications of the confusion matrix. To do this, we should attempt to detect how much the predictions overlap.\n",
    "\n",
    "In classical image processing, this was done via calculating the intersection over union (IoU) between predictions and ground truth. We can adopt the same technique for our 1-D problem (time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the intersection of 2 bouts\n",
    "# Each bout is a tuple of (start, duration)\n",
    "def calculate_intersection(gt_bout, pr_bout):\n",
    "    # Detect the larger of the 2 start times\n",
    "    max_start_time = np.max([gt_bout[0], pr_bout[0]])\n",
    "    # Detect the smaller of the 2 end times\n",
    "    gt_bout_end = gt_bout[0]+gt_bout[1]\n",
    "    pr_bout_end = pr_bout[0]+pr_bout[1]\n",
    "    min_end_time = np.min([gt_bout_end, pr_bout_end])\n",
    "    # Detect if the 2 bouts intersected at all\n",
    "    if max_start_time < min_end_time:\n",
    "        return min_end_time-max_start_time\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Calculates the union of 2 bouts\n",
    "# Each bout is a tuple of (start, duration)\n",
    "def calculate_union(gt_bout, pr_bout):\n",
    "    # If the 2 don't intersect, we can just sum up the durations\n",
    "    if calculate_intersection(gt_bout, pr_bout) == 0:\n",
    "        return gt_bout[1] + pr_bout[1]\n",
    "    # They do intersect\n",
    "    else:\n",
    "        min_start_time = np.min([gt_bout[0], pr_bout[0]])\n",
    "        gt_bout_end = gt_bout[0]+gt_bout[1]\n",
    "        pr_bout_end = pr_bout[0]+pr_bout[1]\n",
    "        max_end_time = np.max([gt_bout_end, pr_bout_end])\n",
    "        return max_end_time - min_start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intersections = []\n",
    "all_unions = []\n",
    "all_ious = []\n",
    "\n",
    "# Loop over the animals again\n",
    "num_test_animals = len(np.unique(test_animals))\n",
    "for animal_idx in np.arange(num_test_animals):\n",
    "    # For each animal, we want a matrix of intersections, unions, and ious\n",
    "    num_gt_bouts = len(all_gt_bouts[animal_idx][0])\n",
    "    num_pr_bouts = len(all_pr_bouts[animal_idx][0])\n",
    "    animal_intersection_mat = np.zeros([num_gt_bouts, num_pr_bouts])\n",
    "    animal_union_mat = np.zeros([num_gt_bouts, num_pr_bouts])\n",
    "    # Do a second loop for each GT bout\n",
    "    for gt_idx in np.arange(num_gt_bouts):\n",
    "        gt_bout = [all_gt_bouts[animal_idx][0][gt_idx], all_gt_bouts[animal_idx][1][gt_idx]]\n",
    "        # Final loop for each proposed bout\n",
    "        for pr_idx in np.arange(num_pr_bouts):\n",
    "            pr_bout = [all_pr_bouts[animal_idx][0][pr_idx], all_pr_bouts[animal_idx][1][pr_idx]]\n",
    "            # Calculate the intersections, unions, and IoUs\n",
    "            animal_intersection_mat[gt_idx, pr_idx] = calculate_intersection(gt_bout, pr_bout)\n",
    "            animal_union_mat[gt_idx, pr_idx] = calculate_union(gt_bout, pr_bout)\n",
    "    # The IoU matrix will just be i / u\n",
    "    animal_iou_mat = animal_intersection_mat/animal_union_mat\n",
    "    # Add the animal data to the resulting lists\n",
    "    all_intersections.append(animal_intersection_mat)\n",
    "    all_unions.append(animal_union_mat)\n",
    "    all_ious.append(animal_iou_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c823d3",
   "metadata": {},
   "source": [
    "Now that we have IoUs, we can try and apply thresholds for calculating things like precision/recall\n",
    "Since it's hard to define \"True Negatives\" with the IoU technique, we can skip that for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbbeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define detection metrics, given a IoU threshold\n",
    "def calc_temporal_iou_metrics(iou_data, threshold):\n",
    "    tp_counts = 0\n",
    "    fn_counts = 0\n",
    "    fp_counts = 0\n",
    "    for cur_iou_mat in iou_data:\n",
    "        tp_counts += np.sum(np.any(cur_iou_mat>threshold, axis=1))\n",
    "        fn_counts += np.sum(np.all(cur_iou_mat<threshold, axis=1))\n",
    "        fp_counts += np.sum(np.all(cur_iou_mat<threshold, axis=0))\n",
    "    precision = tp_counts/(tp_counts + fp_counts)\n",
    "    recall = tp_counts/(tp_counts + fn_counts)\n",
    "    f1 = 2*(precision * recall)/(precision + recall)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_iou_results = []\n",
    "\n",
    "ious_to_evaluate = all_ious\n",
    "\n",
    "for cur_iou in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    performance = calc_temporal_iou_metrics(ious_to_evaluate, cur_iou)\n",
    "    all_iou_results.append(pd.DataFrame({'iou_threshold':[cur_iou], 'precision':[performance[0]], 'recall':[performance[1]], 'f1':[performance[2]]}))\n",
    "\n",
    "iou_df = pd.concat(all_iou_results)\n",
    "iou_df = pd.melt(iou_df, id_vars = ['iou_threshold'], value_vars = ['precision','recall','f1'])\n",
    "\n",
    "iou_plot = p9.ggplot(iou_df) + \\\n",
    "    p9.geom_line(p9.aes(x='iou_threshold', y='value', color='factor(variable)')) + \\\n",
    "    p9.theme_bw()\n",
    "\n",
    "iou_plot.draw().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b18e48",
   "metadata": {},
   "source": [
    "## Discussion 3\n",
    "\n",
    "Pros and Cons to bout based analysis\n",
    "\n",
    "False positives are overestimated, because there may be multiple predicted bouts within 1 real bout.\n",
    "\n",
    "Are there potential methods to try and fix this issue?\n",
    "\n",
    "[Discussion Notes](http://localhost:8888/notebooks/ML-Course-2022-JABS_SOLUTIONS.ipynb#Discussion-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c8a41",
   "metadata": {},
   "source": [
    "## Step 11\n",
    "\n",
    "Post-processing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a933de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_behavior_gaps(bout_starts, bout_durations, bout_states, max_gap_size, state_to_merge = False):\n",
    "    gaps_to_remove = np.logical_and(bout_states==state_to_merge, bout_durations<max_gap_size)\n",
    "    new_durations = np.copy(bout_durations)\n",
    "    new_starts = np.copy(bout_starts)\n",
    "    new_states = np.copy(bout_states)\n",
    "    if np.any(gaps_to_remove):\n",
    "        # Go through backwards removing gaps\n",
    "        for cur_gap in np.where(gaps_to_remove)[0][::-1]:\n",
    "            # Nothing earlier or later to join together, ignore\n",
    "            if cur_gap == 0 or cur_gap == len(new_durations)-1:\n",
    "                pass\n",
    "            else:\n",
    "                cur_duration = np.sum(new_durations[cur_gap-1:cur_gap+2])\n",
    "                new_durations[cur_gap-1] = cur_duration\n",
    "                new_durations = np.delete(new_durations, [cur_gap, cur_gap+1])\n",
    "                new_starts = np.delete(new_starts, [cur_gap, cur_gap+1])\n",
    "                new_states = np.delete(new_states, [cur_gap, cur_gap+1])\n",
    "    return new_starts, new_durations, new_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27886e1d",
   "metadata": {},
   "source": [
    "## Experiment 5\n",
    "\n",
    "Test performance across multiple filtering parameters\n",
    "Run the next 2 cells and change the 2 variables at the top to be more/less stringent on bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdeb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out short bouts\n",
    "min_bout_duration = 9\n",
    "# Remove short breaks in bouts\n",
    "min_gap_duration = 5\n",
    "\n",
    "filtered_pr_bouts = []\n",
    "for animal_id in np.unique(test_animals):\n",
    "    animal_indices = test_animals==animal_id\n",
    "    raw_bout_predictions = rle(test_predictions[animal_indices])\n",
    "    # Remove short breaks in bouts\n",
    "    filtered_bout_predictions = merge_behavior_gaps(raw_bout_predictions[0], raw_bout_predictions[1], raw_bout_predictions[2], min_bout_duration, state_to_merge=False)\n",
    "    # Filter out short bouts\n",
    "    filtered_bout_predictions = merge_behavior_gaps(filtered_bout_predictions[0], filtered_bout_predictions[1], filtered_bout_predictions[2], min_bout_duration, state_to_merge=True)\n",
    "    behavior_bouts = filtered_bout_predictions[2]==1\n",
    "    if np.any(behavior_bouts):\n",
    "        filtered_bout_predictions = (filtered_bout_predictions[0][behavior_bouts], filtered_bout_predictions[1][behavior_bouts])\n",
    "        filtered_pr_bouts.append(filtered_bout_predictions)\n",
    "    else:\n",
    "        filtered_pr_bouts.append(([],[]))\n",
    "\n",
    "filtered_intersections = []\n",
    "filtered_unions = []\n",
    "filtered_ious = []\n",
    "\n",
    "# Loop over the animals again\n",
    "num_test_animals = len(np.unique(test_animals))\n",
    "for animal_idx in np.arange(num_test_animals):\n",
    "    # For each animal, we want a matrix of intersections, unions, and ious\n",
    "    num_gt_bouts = len(all_gt_bouts[animal_idx][0])\n",
    "    num_pr_bouts = len(filtered_pr_bouts[animal_idx][0])\n",
    "    animal_intersection_mat = np.zeros([num_gt_bouts, num_pr_bouts])\n",
    "    animal_union_mat = np.zeros([num_gt_bouts, num_pr_bouts])\n",
    "    # Do a second loop for each GT bout\n",
    "    for gt_idx in np.arange(num_gt_bouts):\n",
    "        gt_bout = [all_gt_bouts[animal_idx][0][gt_idx], all_gt_bouts[animal_idx][1][gt_idx]]\n",
    "        # Final loop for each proposed bout\n",
    "        for pr_idx in np.arange(num_pr_bouts):\n",
    "            pr_bout = [filtered_pr_bouts[animal_idx][0][pr_idx], filtered_pr_bouts[animal_idx][1][pr_idx]]\n",
    "            # Calculate the intersections, unions, and IoUs\n",
    "            animal_intersection_mat[gt_idx, pr_idx] = calculate_intersection(gt_bout, pr_bout)\n",
    "            animal_union_mat[gt_idx, pr_idx] = calculate_union(gt_bout, pr_bout)\n",
    "    # The IoU matrix will just be i / u\n",
    "    animal_iou_mat = animal_intersection_mat/animal_union_mat\n",
    "    # Add the animal data to the resulting lists\n",
    "    filtered_intersections.append(animal_intersection_mat)\n",
    "    filtered_unions.append(animal_union_mat)\n",
    "    filtered_ious.append(animal_iou_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e7f89",
   "metadata": {},
   "source": [
    "Plot the performance with post-processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d7b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_iou_results = []\n",
    "\n",
    "for cur_iou in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    performance = calc_temporal_iou_metrics(filtered_ious, cur_iou)\n",
    "    all_iou_results.append(pd.DataFrame({'iou_threshold':[cur_iou], 'precision':[performance[0]], 'recall':[performance[1]], 'f1':[performance[2]]}))\n",
    "\n",
    "iou_df_filtered = pd.concat(all_iou_results)\n",
    "iou_df_filtered = pd.melt(iou_df_filtered, id_vars = ['iou_threshold'], value_vars = ['precision','recall','f1'])\n",
    "\n",
    "iou_df_filtered['filtered'] = True\n",
    "iou_df['filtered'] = False\n",
    "iou_df_filtered = pd.concat([iou_df_filtered, iou_df])\n",
    "\n",
    "\n",
    "iou_plot = p9.ggplot(iou_df_filtered) + \\\n",
    "    p9.geom_line(p9.aes(x='iou_threshold', y='value', color='filtered')) + \\\n",
    "    p9.facet_wrap('variable') + \\\n",
    "    p9.theme_bw()\n",
    "\n",
    "\n",
    "iou_plot.draw().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "bout_data = []\n",
    "for animal_idx, animal_id in enumerate(np.unique(test_animals)):\n",
    "    gt_bout_data = all_gt_bouts[animal_idx]\n",
    "    if len(gt_bout_data[0])>0:\n",
    "        gt_bout_df = pd.DataFrame({'state':'GT', 'animal':animal_id, 'animal_idx':animal_idx, 'start_time':gt_bout_data[0], 'end_time':gt_bout_data[0]+gt_bout_data[1]})\n",
    "        bout_data.append(gt_bout_df)\n",
    "    pr_bout_data = filtered_pr_bouts[animal_idx]\n",
    "    if len(pr_bout_data[0])>0:\n",
    "        pr_bout_df = pd.DataFrame({'state':'PR', 'animal':animal_id, 'animal_idx':animal_idx, 'start_time':pr_bout_data[0], 'end_time':pr_bout_data[0]+pr_bout_data[1]})\n",
    "        bout_data.append(pr_bout_df)\n",
    "\n",
    "bout_data = pd.concat(bout_data)\n",
    "bout_plot = p9.ggplot(bout_data) + \\\n",
    "    p9.geom_rect(p9.aes(xmin='start_time', xmax='end_time', ymin='animal_idx-0.25', ymax='animal_idx+0.25', fill='factor(state)'), alpha=0.5) + \\\n",
    "    p9.labs(title='Bout annotations and predictions', x='Time, frame', y='Animal index', fill='State') + \\\n",
    "    p9.theme_bw()\n",
    "\n",
    "bout_plot.draw().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
